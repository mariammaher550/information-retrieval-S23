{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Crawler\n",
    "\n",
    "## 1.0. Related example\n",
    "\n",
    "This code shows `wget`-like tool written in python. Run it from console (`python wget.py`), make it work. Check the code, reuse, and modify for your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "\n",
    "\n",
    "def wget(url, filename):\n",
    "    # allow redirects - in case file is relocated\n",
    "    resp = requests.get(url, allow_redirects=True)\n",
    "    # this can also be 2xx, but for simplicity now we stick to 200\n",
    "    # you can also check for `resp.ok`\n",
    "    if resp.status_code != 200:\n",
    "        print(resp.status_code, resp.reason, 'for', url)\n",
    "        return\n",
    "\n",
    "    # just to be cool and print something\n",
    "    print(*[f\"{key}: {value}\" for key, value in resp.headers.items()], sep='\\n')\n",
    "    print()\n",
    "\n",
    "    # try to extract filename from url\n",
    "    if filename is None:\n",
    "        # start with http*, ends if ? or # appears (or none of)\n",
    "        m = re.search(\"^http.*/([^/\\?#]*)[\\?#]?\", url)\n",
    "        filename = m.group(1)\n",
    "        if not filename:\n",
    "            raise NameError(f\"Filename neither given, nor found for {url}\")\n",
    "\n",
    "    # what will you do in case 2 websites store file with the same name?\n",
    "    if os.path.exists(filename):\n",
    "        raise OSError(f\"File {filename} already exists\")\n",
    "\n",
    "    with open(filename, 'wb') as f:\n",
    "        f.write(resp.content)\n",
    "        print(f\"File saved as {filename}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description='download file.')\n",
    "    parser.add_argument(\"-O\", type=str, default=None, dest='filename', help=\"output file name. Default -- taken from resource\")\n",
    "    parser.add_argument(\"url\", type=str, default=None, help=\"http://sprotasov.ru/data/iu.txt\")\n",
    "    args = parser.parse_args()\n",
    "    wget(args.url, args.filename)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0.1. How to parse a page?\n",
    "\n",
    "If you build a crawler, you might follow one of the approaches:\n",
    "1. search for URLs in the page, assuming this is just a text.\n",
    "2. search for URLs in the places where URLs should appear: `<a href=..`, `<img src=...`, `<iframe src=...` and so on.\n",
    "\n",
    "To follow the first approach you can rely on some good regular expression. [Like this](https://stackoverflow.com/a/3809435).\n",
    "\n",
    "To follow the second approach just read one of these: [short answer](https://stackoverflow.com/questions/1080411/retrieve-links-from-web-page-using-python-and-beautifulsoup) or [exhaustive explanation](https://hackersandslackers.com/scraping-urls-with-beautifulsoup/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. [15] Download and persist #\n",
    "Please complete a code for `load()`, `download()` and `persist()` methods of `Document` class. What they do:\n",
    "- for a given URL `download()` method downloads binary data and stores in `self.content`. It returns `True` for success, else `False`.\n",
    "- `persist()` method saves `self.content` somewhere in file system. We do it to avoid multiple downloads (for caching in other words).\n",
    "- `load()` method loads data from hard drive. Returns `True` for success.\n",
    "\n",
    "Tests checks that your code somehow works.\n",
    "\n",
    "**NB Passing the test doesn't mean you correctly completed the task.** These are **criteria, which have to be fullfilled**:\n",
    "1. URL is a unique identifier (as it is a subset of URI). Thus, documents with different URLs should be stored in different files. Typical errors: documents from the same domain are overwritten to the same file, URLs with similar endings are downloaded to the same file, etc.\n",
    "2. The document can be not only a text file, but also a binary. Pay attention that if you download `mp3` file, it still can be played. Hint: don't hurry to convert everything to text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on https://stackoverflow.com/questions/53101597/how-to-download-binary-file-using-requests\n",
    "#based on https://stackoverflow.com/questions/12474406/python-how-to-get-the-content-type-of-an-url\n",
    "\n",
    "import requests\n",
    "from urllib.parse import quote\n",
    "import hashlib\n",
    "import pathlib\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Document:\n",
    "\n",
    "\n",
    "    def __init__(self, url):\n",
    "        self.content = None\n",
    "        self.url = url\n",
    "        self.response = None\n",
    "        self.headers = None\n",
    "\n",
    "    def get_filename(self):\n",
    "        filename = hashlib.sha256(self.url.encode()).hexdigest()\n",
    "        #print(self.headers)\n",
    "        content_type = \"html\"\n",
    "        if \"content-type\" in self.headers:\n",
    "            content_type = self.headers[\"content-type\"]\n",
    "\n",
    "        extension = \"\"\n",
    "        if \"html\" in content_type:\n",
    "            extension = \"html\"\n",
    "        elif \"text\" in content_type:\n",
    "            extension = \"txt\"\n",
    "        else:\n",
    "            extension = content_type.split('/')[-1]\n",
    "        return filename+'.'+extension\n",
    "\n",
    "    def get(self):\n",
    "        if not self.load():\n",
    "            if not self.download():\n",
    "                print(f\"This url : {self.url} can't be obtained due to {self.response.status_code}\")\n",
    "                #raise FileNotFoundError(self.url)\n",
    "            else:\n",
    "                self.persist()\n",
    "\n",
    "    def download(self):\n",
    "        # download self.url content, store it in self.content and return True in case of success\n",
    "            self.response = requests.get(self.url, allow_redirects=True)\n",
    "            if self.response.status_code == 200:\n",
    "                self.content = self.response.content\n",
    "                r = requests.head(self.url)\n",
    "                self.headers = r.headers\n",
    "                return True\n",
    "            return False\n",
    "\n",
    "    def persist(self):\n",
    "        # write document content to hard drive\n",
    "        try:\n",
    "            open(self.get_filename(), \"wb\").write(self.content)\n",
    "        except Exception as e:\n",
    "            print(f\"Error writing content from {self.url} with error {e}\")\n",
    "\n",
    "\n",
    "    def load(self):\n",
    "        # load content from hard drive, store it in self.content and return True in case of success\n",
    "        try :\n",
    "            if self.headers is None:\n",
    "                return False\n",
    "\n",
    "            name = self.get_filename()\n",
    "            with open(name, \"rb\") as file:\n",
    "                self.content = file.readlines()\n",
    "        except Exception as e:\n",
    "            print(self.get_filename())\n",
    "            print(f\"Error loading content from hard drive from {self.url} due to error {e}\")\n",
    "            return False\n",
    "        return True\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1. Tests ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = Document(\"http://sprotasov.ru/data/iu.txt\")\n",
    "\n",
    "doc.get()\n",
    "assert doc.content, \"Document download failed\"\n",
    "assert \"Code snippets, demos and labs for the course\" in str(doc.content), \"Document content error\"\n",
    "\n",
    "doc.get()\n",
    "assert doc.load(), \"Load should return true for saved document\"\n",
    "assert \"Code snippets, demos and labs for the course\" in str(doc.content), \"Document load from disk error\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. [10] Parse HTML\n",
    "`BeautifulSoap` library is a de facto standard to parse XML and HTML documents in python. Use it to complete `parse()` method that extracts document contents. You should initialize:\n",
    "1. `self.anchors` list of tuples `('text', 'url')` met in a document. Be aware, there exist relative links (e.g. `../content/pic.jpg`). Use `urllib.parse.urljoin()` to fix this issue.\n",
    "2. `self.images` list of images met in a document. Again, links can be relative to current page.\n",
    "3. `self.text` should keep plain text of the document without scripts, tags, comments and so on. You can refer to [this stackoverflow answer](https://stackoverflow.com/a/1983219) for details.\n",
    "\n",
    "**NB All these 3 criteria must be fulfilled to get full point for the task.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on https://stackoverflow.com/questions/1080411/retrieve-links-from-web-page-using-python-and-beautifulsoup\n",
    "# Based on https://www.projectpro.io/recipes/download-image-from-webpage-beautiful-soup\n",
    "\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "from bs4.element import Comment\n",
    "from urllib.parse import urljoin\n",
    "import validators\n",
    "\n",
    "def is_valid(link):\n",
    "    return type(link) is str and validators.url(link)\n",
    "\n",
    "\n",
    "class HtmlDocument(Document):\n",
    "\n",
    "    def __init__(self, url):\n",
    "        super().__init__(url)\n",
    "\n",
    "    def parse(self):\n",
    "        self.anchors = []\n",
    "\n",
    "        for link in BeautifulSoup(self.response.text, parse_only=SoupStrainer('a')):\n",
    "            if link.has_attr('href'):\n",
    "                if \"http\" not in link['href'] and is_valid(urljoin(self.url,link['href'])):\n",
    "                    self.anchors.append((link.text,urljoin(self.url,link['href'])))\n",
    "                elif is_valid(link['href']):\n",
    "                    self.anchors.append((link.text,link['href']))\n",
    "\n",
    "        self.images = []\n",
    "\n",
    "        for link in BeautifulSoup(self.response.text, parse_only=SoupStrainer('img')):\n",
    "            if link.has_attr('src'):\n",
    "                if \"http\" not in link['src']:\n",
    "                 self.images.append(urljoin(self.url ,link['src']))\n",
    "                else:\n",
    "                    self.images.append(link['src'])\n",
    "\n",
    "        self.text = BeautifulSoup(self.response.text, 'html.parser').text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1. Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = HtmlDocument(\"http://sprotasov.ru\")\n",
    "doc.get()\n",
    "doc.parse()\n",
    "\n",
    "assert \"just few links\" in doc.text, \"Error parsing text\"\n",
    "assert \"http://sprotasov.ru/images/gb.svg\" in doc.images, \"Error parsing images\"\n",
    "assert any(p[1] == \"https://twitter.com/07C3\" for p in doc.anchors), \"Error parsing links\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. [10] Document analysis ##\n",
    "Complete the code for `HtmlDocumentTextData` class. Implement word and sentence splitting (use any method you can propose). \n",
    "\n",
    "**Criteria to succeed in the task**: \n",
    "1. Your `get_word_stats()` method should return `Counter` object.\n",
    "2. Don't forget to lowercase your words for counting.\n",
    "3. Sentences should be obtained from inside `<body>` tag only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on https://www.geeksforgeeks.org/how-to-scrape-all-the-text-from-body-tag-using-beautifulsoup-in-python/\n",
    "\n",
    "# based on https://www.reddit.com/r/learnpython/comments/1928vp/errors_in_my_downloader_script_giving_me_trouble/\n",
    "\n",
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "\n",
    "class HtmlDocumentTextData:\n",
    "    \n",
    "    def __init__(self, url):\n",
    "        self.doc = HtmlDocument(url)\n",
    "        self.doc.get()\n",
    "        self.doc.parse()\n",
    "\n",
    "    \n",
    "    def get_sentences(self):\n",
    "\n",
    "        soup = BeautifulSoup(self.doc.response.content, \"html.parser\")\n",
    "\n",
    "        # Get the whole body tag\n",
    "\n",
    "        tag = soup.body\n",
    "        result =[]\n",
    "\n",
    "        for s in tag.strings:\n",
    "\n",
    "           # new_string = s.translate(str.maketrans('','',string.punctuation))\n",
    "            new_string = re.sub(r'[^\\w\\s]', '', s)\n",
    "            result.append(new_string)\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def get_word_stats(self):\n",
    "        #TODO return Counter object of the document, containing mapping {`word` -> count_in_doc}\n",
    "        sentences = self.get_sentences()\n",
    "        words = []\n",
    "        for sentence in sentences:\n",
    "            for word in sentence.split():\n",
    "                words.append(word.lower())\n",
    "\n",
    "        #print(words)\n",
    "        words_counter = Counter(words)\n",
    "        return words_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1. Tests ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('и', 44), ('в', 22), ('иннополис', 20), ('с', 13), ('на', 12), ('университет', 11), ('университета', 11), ('центр', 10), ('для', 9), ('образование', 8)]\n"
     ]
    }
   ],
   "source": [
    "doc = HtmlDocumentTextData(\"https://innopolis.university/\")\n",
    "\n",
    "doc.get_sentences()\n",
    "print(doc.get_word_stats().most_common(10))\n",
    "assert [x for x in doc.get_word_stats().most_common(10) if x[0] == 'иннополис'], 'иннополис should be among most common'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. [15] Crawling ##\n",
    "\n",
    "Method `crawl_generator()` is given starting url (`source`) and max depth of search. It should return a **generator** of `HtmlDocumentTextData` objects (return a document as soon as it is downloaded and parsed). You can benefit from `yield obj_name` python construction. Use `HtmlDocumentTextData.anchors` field to go deeper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on https://www.geeksforgeeks.org/web-crawling-using-breadth-first-search-at-a-specified-depth/\n",
    "\n",
    "from queue import Queue\n",
    "\n",
    "class Crawler:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.intern_extern_links = set()\n",
    "        self.visited = {}\n",
    "\n",
    "\n",
    "    def crawl_generator(self, source, depth=1):\n",
    "        if depth == 0:\n",
    "            src = HtmlDocumentTextData(source)\n",
    "            if src.doc.response.status_code != 200:\n",
    "                print(f\"skipping {source} due to {src.doc.response.status_code}\")\n",
    "                return\n",
    "            elif src.doc.get_filename()[-4:] in ('.pdf', '.mp3', '.avi', '.mp4', '.txt'):\n",
    "                return\n",
    "            yield src\n",
    "\n",
    "        elif depth == 1:\n",
    "            src = HtmlDocumentTextData(source)\n",
    "            links = src.doc.anchors\n",
    "            for link in links:\n",
    "                current_doc = HtmlDocumentTextData(link)\n",
    "                if  current_doc.doc.response.status_code != 200:\n",
    "                        print(f\"skipping {current_doc.doc.url} due to {current_doc.doc.response.status_code}\")\n",
    "                        continue\n",
    "                elif current_doc.doc.get_filename()[-4:] in ('.pdf', '.mp3', '.avi', '.mp4', '.txt'):\n",
    "                        print(f\"skipping {current_doc.doc.url} don't like its extension\")\n",
    "                        continue\n",
    "                yield  current_doc\n",
    "\n",
    "        else:\n",
    "            queue = [source]\n",
    "            for i in range(depth):\n",
    "                for j in range(len(queue)):\n",
    "                    link = queue.pop(0)\n",
    "                    current_doc = HtmlDocumentTextData(link)\n",
    "                    if  current_doc.doc.response.status_code != 200:\n",
    "                        print(f\"skipping {current_doc.doc.url} due to {current_doc.doc.response.status_code}\")\n",
    "                        continue\n",
    "                    elif current_doc.doc.get_filename()[-4:] in ('.pdf', '.mp3', '.avi', '.mp4', '.txt'):\n",
    "                        print(f\"skipping {current_doc.doc.url} don't like its extension\")\n",
    "                        continue\n",
    "                    urls = current_doc.doc.anchors\n",
    "                    for u in urls:\n",
    "                        if not (u in self.visited):\n",
    "                            queue.append(u)\n",
    "                            self.visited[u] = True\n",
    "                    yield current_doc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.1. Tests ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://innopolis.university/\n",
      "575 distinct word(s) so far\n"
     ]
    },
    {
     "ename": "InvalidSchema",
     "evalue": "No connection adapters were found for \"('\\\\n\\\\n', 'https://innopolis.university/')\"",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mInvalidSchema\u001B[0m                             Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[18], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m crawler \u001B[38;5;241m=\u001B[39m Crawler()\n\u001B[0;32m      2\u001B[0m counter \u001B[38;5;241m=\u001B[39m Counter()\n\u001B[1;32m----> 4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m crawler\u001B[38;5;241m.\u001B[39mcrawl_generator(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://innopolis.university/\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m2\u001B[39m):\n\u001B[0;32m      5\u001B[0m     \u001B[38;5;28mprint\u001B[39m(c\u001B[38;5;241m.\u001B[39mdoc\u001B[38;5;241m.\u001B[39murl)\n\u001B[0;32m      7\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m c\u001B[38;5;241m.\u001B[39mdoc\u001B[38;5;241m.\u001B[39murl[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m4\u001B[39m:] \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.pdf\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.mp3\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.avi\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.mp4\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.txt\u001B[39m\u001B[38;5;124m'\u001B[39m):\n",
      "Cell \u001B[1;32mIn[17], line 40\u001B[0m, in \u001B[0;36mCrawler.crawl_generator\u001B[1;34m(self, source, depth)\u001B[0m\n\u001B[0;32m     38\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m j \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(queue)):\n\u001B[0;32m     39\u001B[0m     link \u001B[38;5;241m=\u001B[39m queue\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m---> 40\u001B[0m     current_doc \u001B[38;5;241m=\u001B[39m \u001B[43mHtmlDocumentTextData\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlink\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     41\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m  current_doc\u001B[38;5;241m.\u001B[39mdoc\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m200\u001B[39m:\n\u001B[0;32m     42\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mskipping \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcurrent_doc\u001B[38;5;241m.\u001B[39mdoc\u001B[38;5;241m.\u001B[39murl\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m due to \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcurrent_doc\u001B[38;5;241m.\u001B[39mdoc\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mstatus_code\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[9], line 13\u001B[0m, in \u001B[0;36mHtmlDocumentTextData.__init__\u001B[1;34m(self, url)\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, url):\n\u001B[0;32m     12\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdoc \u001B[38;5;241m=\u001B[39m HtmlDocument(url)\n\u001B[1;32m---> 13\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdoc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     14\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdoc\u001B[38;5;241m.\u001B[39mparse()\n",
      "Cell \u001B[1;32mIn[2], line 40\u001B[0m, in \u001B[0;36mDocument.get\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     38\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m     39\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mload():\n\u001B[1;32m---> 40\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdownload\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[0;32m     41\u001B[0m             \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThis url : \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39murl\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m can\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt be obtained due to \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mstatus_code\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     42\u001B[0m             \u001B[38;5;66;03m#raise FileNotFoundError(self.url)\u001B[39;00m\n\u001B[0;32m     43\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "Cell \u001B[1;32mIn[2], line 48\u001B[0m, in \u001B[0;36mDocument.download\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     46\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdownload\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m     47\u001B[0m     \u001B[38;5;66;03m# download self.url content, store it in self.content and return True in case of success\u001B[39;00m\n\u001B[1;32m---> 48\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresponse \u001B[38;5;241m=\u001B[39m \u001B[43mrequests\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mallow_redirects\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m     49\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m200\u001B[39m:\n\u001B[0;32m     50\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontent \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mcontent\n",
      "File \u001B[1;32m~\\GitHub\\information-retrieval\\venv\\lib\\site-packages\\requests\\api.py:73\u001B[0m, in \u001B[0;36mget\u001B[1;34m(url, params, **kwargs)\u001B[0m\n\u001B[0;32m     62\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget\u001B[39m(url, params\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m     63\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Sends a GET request.\u001B[39;00m\n\u001B[0;32m     64\u001B[0m \n\u001B[0;32m     65\u001B[0m \u001B[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     70\u001B[0m \u001B[38;5;124;03m    :rtype: requests.Response\u001B[39;00m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m---> 73\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m request(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mget\u001B[39m\u001B[38;5;124m\"\u001B[39m, url, params\u001B[38;5;241m=\u001B[39mparams, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\GitHub\\information-retrieval\\venv\\lib\\site-packages\\requests\\api.py:59\u001B[0m, in \u001B[0;36mrequest\u001B[1;34m(method, url, **kwargs)\u001B[0m\n\u001B[0;32m     55\u001B[0m \u001B[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001B[39;00m\n\u001B[0;32m     56\u001B[0m \u001B[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001B[39;00m\n\u001B[0;32m     57\u001B[0m \u001B[38;5;66;03m# cases, and look like a memory leak in others.\u001B[39;00m\n\u001B[0;32m     58\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m sessions\u001B[38;5;241m.\u001B[39mSession() \u001B[38;5;28;01mas\u001B[39;00m session:\n\u001B[1;32m---> 59\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m session\u001B[38;5;241m.\u001B[39mrequest(method\u001B[38;5;241m=\u001B[39mmethod, url\u001B[38;5;241m=\u001B[39murl, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\GitHub\\information-retrieval\\venv\\lib\\site-packages\\requests\\sessions.py:587\u001B[0m, in \u001B[0;36mSession.request\u001B[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001B[0m\n\u001B[0;32m    582\u001B[0m send_kwargs \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m    583\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtimeout\u001B[39m\u001B[38;5;124m\"\u001B[39m: timeout,\n\u001B[0;32m    584\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mallow_redirects\u001B[39m\u001B[38;5;124m\"\u001B[39m: allow_redirects,\n\u001B[0;32m    585\u001B[0m }\n\u001B[0;32m    586\u001B[0m send_kwargs\u001B[38;5;241m.\u001B[39mupdate(settings)\n\u001B[1;32m--> 587\u001B[0m resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msend(prep, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39msend_kwargs)\n\u001B[0;32m    589\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m resp\n",
      "File \u001B[1;32m~\\GitHub\\information-retrieval\\venv\\lib\\site-packages\\requests\\sessions.py:695\u001B[0m, in \u001B[0;36mSession.send\u001B[1;34m(self, request, **kwargs)\u001B[0m\n\u001B[0;32m    692\u001B[0m hooks \u001B[38;5;241m=\u001B[39m request\u001B[38;5;241m.\u001B[39mhooks\n\u001B[0;32m    694\u001B[0m \u001B[38;5;66;03m# Get the appropriate adapter to use\u001B[39;00m\n\u001B[1;32m--> 695\u001B[0m adapter \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_adapter\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    697\u001B[0m \u001B[38;5;66;03m# Start time (approximately) of the request\u001B[39;00m\n\u001B[0;32m    698\u001B[0m start \u001B[38;5;241m=\u001B[39m preferred_clock()\n",
      "File \u001B[1;32m~\\GitHub\\information-retrieval\\venv\\lib\\site-packages\\requests\\sessions.py:792\u001B[0m, in \u001B[0;36mSession.get_adapter\u001B[1;34m(self, url)\u001B[0m\n\u001B[0;32m    789\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m adapter\n\u001B[0;32m    791\u001B[0m \u001B[38;5;66;03m# Nothing matches :-/\u001B[39;00m\n\u001B[1;32m--> 792\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m InvalidSchema(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo connection adapters were found for \u001B[39m\u001B[38;5;132;01m{\u001B[39;00murl\u001B[38;5;132;01m!r}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mInvalidSchema\u001B[0m: No connection adapters were found for \"('\\\\n\\\\n', 'https://innopolis.university/')\""
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "crawler = Crawler()\n",
    "counter = Counter()\n",
    "\n",
    "for c in crawler.crawl_generator(\"https://innopolis.university/\", 2):\n",
    "    print(c.doc.url)\n",
    "\n",
    "    if c.doc.url[-4:] in ('.pdf', '.mp3', '.avi', '.mp4', '.txt'):\n",
    "        print(\"Skipping\", c.doc.url)\n",
    "        continue\n",
    "    counter.update(c.get_word_stats())\n",
    "    print(len(counter), \"distinct word(s) so far\")\n",
    "    \n",
    "print(\"Done\")\n",
    "\n",
    "print(counter.most_common(20))\n",
    "assert [x for x in counter.most_common(20) if x[0] == 'innopolis'], 'innopolis sould be among most common'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting validators\n",
      "  Using cached validators-0.20.0.tar.gz (30 kB)\n",
      "Requirement already satisfied: decorator>=3.4.0 in c:\\users\\maria\\appdata\\roaming\\python\\python310\\site-packages (from validators) (5.1.1)\n",
      "Building wheels for collected packages: validators\n",
      "  Building wheel for validators (setup.py): started\n",
      "  Building wheel for validators (setup.py): finished with status 'done'\n",
      "  Created wheel for validators: filename=validators-0.20.0-py3-none-any.whl size=19582 sha256=f9975a88f9a17f32714268afb4945d427a221b539e35a9d5774ad52173f6039a\n",
      "  Stored in directory: c:\\users\\maria\\appdata\\local\\pip\\cache\\wheels\\f2\\ed\\dd\\d3a556ad245ef9dc570c6bcd2f22886d17b0b408dd3bbb9ac3\n",
      "Successfully built validators\n",
      "Installing collected packages: validators\n",
      "Successfully installed validators-0.20.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: You are using pip version 21.2.4; however, version 23.0 is available.\n",
      "You should consider upgrading via the 'C:\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting httplib2\n",
      "  Using cached httplib2-0.21.0-py3-none-any.whl (96 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\python310\\lib\\site-packages (from httplib2) (3.0.8)\n",
      "Installing collected packages: httplib2\n",
      "Successfully installed httplib2-0.21.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: You are using pip version 21.2.4; however, version 23.0 is available.\n",
      "You should consider upgrading via the 'C:\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "#Installing libraries\n",
    "!pip install validators\n",
    "!pip install httplib2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
